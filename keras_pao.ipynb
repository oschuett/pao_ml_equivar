{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import itertools\n",
    "from pao_file_utils import parse_pao_file\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spherical_harmonics import Y_l, get_clebsch_gordan_coefficients_sympy\n",
    "get_clebsch_gordan_coefficients = get_clebsch_gordan_coefficients_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolute(coords, atom2kind, central_atom, max_l):\n",
    "    natoms = coords.shape[0]\n",
    "    assert coords.shape[1] == 3\n",
    "    kind_names = list(sorted(set(atom2kind)))\n",
    "    integrals = []\n",
    "    for l in range(max_l + 1):\n",
    "        for sigma in [0.5, 1.0, 2.0, 3.0, 4.0]:\n",
    "            for ikind in kind_names:\n",
    "                integrals.append(np.zeros(2*l + 1))\n",
    "                for iatom in range(natoms):\n",
    "                    if atom2kind[iatom] == ikind and iatom != central_atom:\n",
    "                        r = coords[central_atom] - coords[iatom]\n",
    "                        angular_part = Y_l(r, l)\n",
    "                        radial_part = np.exp(- np.dot(r,r) / sigma**2)\n",
    "                        integrals[-1] += radial_part * angular_part\n",
    "    return integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(channels, max_l):\n",
    "    \"\"\" returns all possbile combinations of input channels up to given max_l \"\"\"\n",
    "    output_channels = list()\n",
    "    for channel_i, channel_j in itertools.combinations_with_replacement(channels, 2):\n",
    "        assert len(channel_i.shape) == len(channel_j.shape) == 1\n",
    "        li = (channel_i.size - 1) // 2\n",
    "        lj = (channel_j.size - 1) // 2\n",
    "        # There li + lj possible ways to combine the two channels.\n",
    "        # We do all of them up to a max_l.\n",
    "        lo_min = abs(li-lj)\n",
    "        lo_max = min(li+lj, max_l)\n",
    "        for lo in range(lo_min, lo_max + 1): # l of output\n",
    "            channel_o = np.zeros(2*lo+1)\n",
    "            cg = get_clebsch_gordan_coefficients(li, lj, lo)\n",
    "            channel_o = np.einsum(\"i,j,ijo->o\", channel_i, channel_j, cg)\n",
    "            output_channels.append(channel_o)\n",
    "    return output_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and hard code metadata.\n",
    "#pao_files = sorted(glob(\"2H2O_MD/frame_*/2H2O_pao44-1_0.pao\"))\n",
    "#pao_files = sorted(glob(\"2H2O_rotations/phi_*/2H2O_pao44-1_0.pao\"))\n",
    "pao_files = sorted(glob(\"2H2O_rotations/*/2H2O_pao44-1_0.pao\"))\n",
    "\n",
    "prim_basis_shells = {\n",
    "    'H': [2, 1, 0], # two s-shells, one p-shell, no d-shells\n",
    "    'O': [2, 2, 1], # two s-shells, two p-shells, one d-shell\n",
    "}\n",
    "\n",
    "pao_basis_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset for:  H\n",
      "samples:  224\n",
      "s channels:  10\n",
      "p channels:  10\n",
      "d channels:  0\n",
      "\n",
      "Dataset for:  O\n",
      "samples:  112\n",
      "s channels:  10\n",
      "p channels:  10\n",
      "d channels:  10\n"
     ]
    }
   ],
   "source": [
    "class Sample:\n",
    "    def __init__(self, channels, xblock, iatom, coords):\n",
    "        self.channels = channels\n",
    "        self.xblock = xblock\n",
    "        self.iatom = iatom # just for debugging\n",
    "        self.coords = coords # just for debugging\n",
    "        #TODO generalize\n",
    "        self.s_channel_list = [c for c in channels if c.size == 1]\n",
    "        self.p_channel_list = [c for c in channels if c.size == 3]\n",
    "        self.d_channel_list = [c for c in channels if c.size == 5]\n",
    "\n",
    "        self.s_channels = np.concatenate(self.s_channel_list)\n",
    "        self.p_channels = np.stack(self.p_channel_list).T\n",
    "        if (self.d_channel_list):\n",
    "            self.d_channels = np.stack(self.d_channel_list).T\n",
    "        else:\n",
    "            self.d_channels = np.zeros(shape=[])\n",
    "\n",
    "def build_dataset(kind_name, max_l):\n",
    "    samples = []\n",
    "    for fn in pao_files:\n",
    "        kinds, atom2kind, coords, xblocks = parse_pao_file(fn)\n",
    "        natoms = coords.shape[0]\n",
    "        for iatom in range(natoms):\n",
    "            if atom2kind[iatom] == kind_name:\n",
    "                initial_channels = convolute(coords, atom2kind, iatom, max_l)\n",
    "                #comb_channels = combinations(initial_channels, max_l)\n",
    "                #sample = Sample(comb_channels, xblocks[iatom], iatom, coords)\n",
    "                sample = Sample(initial_channels, xblocks[iatom], iatom, coords)\n",
    "                samples.append(sample)\n",
    "\n",
    "    print(\"\\nDataset for: \", kind_name)\n",
    "    print(\"samples: \", len(samples))\n",
    "    print(\"s channels: \", len(samples[0].s_channel_list))\n",
    "    print(\"p channels: \", len(samples[0].p_channel_list))\n",
    "    print(\"d channels: \", len(samples[0].d_channel_list))\n",
    "    return samples\n",
    "    \n",
    "def samples2xy(samples):\n",
    "    s_input = np.array([s.s_channels for s in samples], np.float32)\n",
    "    p_input = np.array([s.p_channels for s in samples], np.float32)\n",
    "    d_input = np.array([s.d_channels for s in samples], np.float32)\n",
    "    x_data = [s_input, p_input, d_input]\n",
    "    y_data = np.array([s.xblock for s in samples], np.float32)\n",
    "    return x_data, y_data\n",
    "\n",
    "H_dataset = build_dataset(\"H\", max_l=1)\n",
    "O_dataset = build_dataset(\"O\", max_l=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement custom layer instead of Dense + Lambda:\n",
    "def contract(args):\n",
    "    assert len(args) == 2\n",
    "    coeffs = args[0]\n",
    "    channels = args[1]\n",
    "    return tf.reduce_sum(coeffs[...,None,:] * channels, axis=-1)\n",
    "\n",
    "def build_model(first_sample, pao_basis_size, prim_basis_shells, num_hidden_layers, hidden_layers_width):\n",
    "    \n",
    "    prim_basis_size = sum([(2*l+1)*n for l, n in enumerate(prim_basis_shells)])\n",
    "    num_p_channels = len(first_sample.p_channel_list)\n",
    "    num_d_channels = len(first_sample.d_channel_list)\n",
    "    \n",
    "    # define two sets of inputs\n",
    "    s_input = layers.Input(shape=first_sample.s_channels.shape, name=\"s_input\")\n",
    "    p_input = layers.Input(shape=first_sample.p_channels.shape, name=\"p_input\")\n",
    "    d_input = layers.Input(shape=first_sample.d_channels.shape, name=\"d_input\")\n",
    "    \n",
    "    #TODO: add more hidden layers\n",
    "    hidden_out = s_input\n",
    "    for i in range(num_hidden_layers):\n",
    "        hidden_out = layers.Dense(hidden_layers_width, activation='softmax', name=f\"hidden_layer_{i}\")(hidden_out)\n",
    "            \n",
    "    pao_output = []\n",
    "    for i in range(pao_basis_size):\n",
    "        for l, n in enumerate(prim_basis_shells):\n",
    "            for j in range(n):\n",
    "                name = f\"{i}{l}{j}\"\n",
    "                if l == 0:\n",
    "                    s_out = layers.Dense(1, name=\"coeffs_\"+name)(hidden_out)\n",
    "                    pao_output.append(s_out)\n",
    "                elif l == 1:\n",
    "                    p_coeffs = layers.Dense(num_p_channels, name=\"coeffs_\"+name)(hidden_out)\n",
    "                    contract_layer = layers.Lambda(contract, name=\"contr_\"+name)\n",
    "                    p_out = contract_layer([p_coeffs, p_input])\n",
    "                    pao_output.append(p_out)\n",
    "                elif l == 2:\n",
    "                    d_coeffs = layers.Dense(num_d_channels, name=\"coeffs_\"+name)(hidden_out)\n",
    "                    contract_layer = layers.Lambda(contract, name=\"contr_\"+name)\n",
    "                    d_out = contract_layer([d_coeffs, d_input])\n",
    "                    pao_output.append(d_out)\n",
    "                else:\n",
    "                    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    xvec = layers.concatenate(pao_output)\n",
    "    xblock = layers.Reshape((pao_basis_size, prim_basis_size))(xvec) #TODO: maybe transpose?\n",
    "\n",
    "    inputs = [s_input, p_input, d_input]\n",
    "    model = keras.Model(inputs=inputs, outputs=xblock)\n",
    "    #model.summary()\n",
    "    return(model)\n",
    "\n",
    "H_model = build_model(H_dataset[0], pao_basis_size, prim_basis_shells['H'], num_hidden_layers=10, hidden_layers_width=10)\n",
    "O_model = build_model(O_dataset[0], pao_basis_size, prim_basis_shells['O'], num_hidden_layers=10, hidden_layers_width=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(xblock_true, xblock_pred):\n",
    "    #TODO: This might not be ideal as it implicitly forces the predicted basis vectors to be orthonormal.\n",
    "    projector = tf.matmul(xblock_pred, xblock_pred, transpose_a=True)\n",
    "    residual = xblock_true - tf.matmul(xblock_true, projector)\n",
    "    return tf.reduce_mean(tf.pow(residual, 2))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "H_model.compile(optimizer, loss=loss_function)\n",
    "O_model.compile(optimizer, loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 Loss: 0.03655224293470383\n",
      "Epoch: 100 Loss: 0.01957555301487446\n",
      "Epoch: 150 Loss: 0.01923091895878315\n",
      "Epoch: 200 Loss: 0.019229192286729813\n",
      "Epoch: 250 Loss: 0.019229184836149216\n",
      "Epoch: 300 Loss: 0.019229186698794365\n",
      "Epoch: 350 Loss: 0.019229218363761902\n",
      "Epoch: 400 Loss: 0.01922922022640705\n",
      "Epoch: 450 Loss: 0.019229186698794365\n",
      "Epoch: 500 Loss: 0.019229186698794365\n",
      "Epoch: 550 Loss: 0.019229458644986153\n",
      "Epoch: 600 Loss: 0.019229186698794365\n",
      "Epoch: 650 Loss: 0.01922968588769436\n",
      "Epoch: 700 Loss: 0.019229192286729813\n",
      "Epoch: 750 Loss: 0.019229255616664886\n",
      "Epoch: 800 Loss: 0.01922920159995556\n",
      "Epoch: 850 Loss: 0.019229469820857048\n",
      "Epoch: 900 Loss: 0.019229857251048088\n",
      "Epoch: 950 Loss: 0.019229233264923096\n",
      "Epoch: 1000 Loss: 0.019229236990213394\n"
     ]
    }
   ],
   "source": [
    "class TrainingLogger(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_counter = 0\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_counter += 1\n",
    "        if self.epoch_counter % 50 == 0:\n",
    "            print(\"Epoch: {} Loss: {}\".format(self.epoch_counter, logs['loss']))\n",
    "\n",
    "#H1_dataset = [s for s in H_dataset if s.iatom==1]\n",
    "#x_train, y_train = samples2xy(H1_dataset[0:1])\n",
    "#H_model.fit(x_train, y_train, epochs=1000, verbose=0, callbacks=[TrainingLogger()]);\n",
    "\n",
    "O3_dataset = [s for s in O_dataset if s.iatom==3]\n",
    "x_train, y_train = samples2xy(O3_dataset[0:1])\n",
    "O_model.fit(x_train, y_train, epochs=1000, verbose=0, callbacks=[TrainingLogger()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H1_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-783d4310e247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mH1_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples2xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'H1_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for s in H1_dataset:\n",
    "    x_test, y_test = samples2xy([s])\n",
    "    loss = H_model.evaluate(x_test, y_test, verbose=0)\n",
    "    losses.append(loss)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test, y_test = samples2xy(O3_dataset[1:2])\n",
    "#print(O_model.predict(x_test)[0,0,:])\n",
    "print(O_model.predict(x_train)[0,0,:])\n",
    "#print(y_test[0,0,:])\n",
    "print(y_train[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(H_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
