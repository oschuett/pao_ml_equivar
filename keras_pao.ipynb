{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import itertools\n",
    "from pao_file_utils import parse_pao_file\n",
    "from sympy.physics.quantum.cg import CG\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/cp2k/cp2k/blob/master/src/common/spherical_harmonics.F\n",
    "def Y_l(r, l):\n",
    "    \"\"\"Real Spherical Harmonics\"\"\"\n",
    "    assert r.shape[-1] == 3\n",
    "\n",
    "    if l < 0:\n",
    "        raise Exceptoin(\"Negative l value\")\n",
    "    elif l == 0:\n",
    "        return np.sqrt(1.0 / (4.0 * np.pi))\n",
    "    elif l == 1:\n",
    "        pf = np.sqrt(3.0 / (4.0 * np.pi))\n",
    "        return pf * r\n",
    "    elif l == 2:\n",
    "        x = r[..., 0]\n",
    "        y = r[..., 1]\n",
    "        z = r[..., 2]\n",
    "        result = np.zeros(5)\n",
    "        # m = 2\n",
    "        pf = np.sqrt(15.0 / (16.0 * np.pi))\n",
    "        result[0] = pf * x**2 - y**2\n",
    "        # m = 1\n",
    "        pf = np.sqrt(15.0 / (4.0 * np.pi))\n",
    "        result[1] = pf * z * x\n",
    "        # m = 0\n",
    "        pf = np.sqrt(5.0 / (16.0 * np.pi))\n",
    "        result[2] = pf * (3.0 * z**2 - 1.0)\n",
    "        # m = -1\n",
    "        pf = np.sqrt(15.0 / (4.0 * np.pi))\n",
    "        result[3] = pf * z * y\n",
    "        # m = -2\n",
    "        pf = np.sqrt(15.0 / (16.0 * np.pi))\n",
    "        result[4] = pf * 2.0 * x * y\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolute(coords, atom2kind, central_atom, max_l):\n",
    "    natoms = coords.shape[0]\n",
    "    assert coords.shape[1] == 3\n",
    "    kind_names = list(sorted(set(atom2kind)))\n",
    "    integrals = []\n",
    "    for l in range(max_l + 1):\n",
    "        for sigma in [0.5, 1.0, 2.0, 3.0, 4.0]:\n",
    "            for ikind in kind_names:\n",
    "                integrals.append(np.zeros(2*l + 1))\n",
    "                for iatom in range(natoms):\n",
    "                    if atom2kind[iatom] == ikind and iatom != central_atom:\n",
    "                        r = coords[central_atom] - coords[iatom]\n",
    "                        angular_part = Y_l(r, l)\n",
    "                        radial_part = np.exp(- np.dot(r,r) / sigma**2)\n",
    "                        integrals[-1] += radial_part * angular_part\n",
    "    return integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_cache = dict()\n",
    "\n",
    "#TODO maybe rename lo -> lk ?\n",
    "def get_clebsch_gordan_coefficients(li, lj, lo):\n",
    "    global cg_cache\n",
    "    key = (li, lj, lo)\n",
    "    if key not in cg_cache:\n",
    "        assert abs(li-lj) <= lo <= abs(li+lj)\n",
    "        coeffs = np.zeros(shape=(2*li+1, 2*lj+1, 2*lo+1))\n",
    "        for mi in range(-li, li+1):\n",
    "            for mj in range(-lj, lj+1):\n",
    "                for mo in range(-lo, lo+1):\n",
    "                    # https://docs.sympy.org/latest/modules/physics/quantum/cg.html\n",
    "                    cg = CG(li, mi, lj, mj, lo, mo).doit()\n",
    "                    coeffs[mi+li, mj+lj, mo+lo] = cg\n",
    "        cg_cache[key] = coeffs\n",
    "    return cg_cache[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(channels, max_l):\n",
    "    \"\"\" returns all possbile combinations of input channels up to given max_l \"\"\"\n",
    "    output_channels = list()\n",
    "    for channel_i, channel_j in itertools.combinations_with_replacement(channels, 2):\n",
    "        assert len(channel_i.shape) == len(channel_j.shape) == 1\n",
    "        li = (channel_i.size - 1) // 2\n",
    "        lj = (channel_j.size - 1) // 2\n",
    "        # There li + lj possible ways to combine the two channels.\n",
    "        # We do all of them up to a max_l.\n",
    "        lo_min = abs(li-lj)\n",
    "        lo_max = min(li+lj, max_l)\n",
    "        for lo in range(lo_min, lo_max + 1): # l of output\n",
    "            channel_o = np.zeros(2*lo+1)\n",
    "            cg = get_clebsch_gordan_coefficients(li, lj, lo)\n",
    "            channel_o = np.einsum(\"i,j,ijo->o\", channel_i, channel_j, cg)\n",
    "            output_channels.append(channel_o)\n",
    "    return output_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and hard code metadata.\n",
    "#pao_files = sorted(glob(\"2H2O_MD/frame_*/2H2O_pao44-1_0.pao\"))\n",
    "pao_files = sorted(glob(\"2H2O_rotations/*/2H2O_pao44-1_0.pao\"))\n",
    "\n",
    "prim_basis_shells = {\n",
    "    'H': [2, 1, 0], # two s-shells, one p-shell, no d-shells\n",
    "    'O': [2, 2, 1], # two s-shells, two p-shells, one d-shell\n",
    "}\n",
    "\n",
    "pao_basis_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset for:  H\n",
      "samples:  224\n",
      "s channels:  110\n",
      "p channels:  155\n",
      "d channels:  0\n"
     ]
    }
   ],
   "source": [
    "class Sample:\n",
    "    def __init__(self, channels, xblock):\n",
    "        self.channels = channels\n",
    "        self.xblock = xblock\n",
    "        #TODO generalize\n",
    "        self.s_channel_list = [c for c in channels if c.size == 1]\n",
    "        self.p_channel_list = [c for c in channels if c.size == 3]\n",
    "        self.d_channel_list = [c for c in channels if c.size == 5]\n",
    "\n",
    "        self.s_channels = np.concatenate(self.s_channel_list)\n",
    "        self.p_channels = np.stack(self.p_channel_list).T\n",
    "        if (self.d_channel_list):\n",
    "            self.d_channels = np.stack(self.d_channel_list).T\n",
    "        else:\n",
    "            self.d_channels = np.zeros(shape=[])\n",
    "\n",
    "def build_dataset(kind_name, max_l):\n",
    "    samples = []\n",
    "    for fn in pao_files:\n",
    "        kinds, atom2kind, coords, xblocks = parse_pao_file(fn)\n",
    "        #kind_onehot = encode_kind(atom2kind)\n",
    "        natoms = coords.shape[0]\n",
    "        for iatom in range(natoms):\n",
    "            if atom2kind[iatom] == kind_name:\n",
    "                initial_channels = convolute(coords, atom2kind, iatom, max_l)\n",
    "                comb_channels = combinations(initial_channels, max_l)\n",
    "                sample = Sample(comb_channels, xblocks[iatom])\n",
    "                samples.append(sample)\n",
    "\n",
    "    print(\"\\nDataset for: \", kind_name)\n",
    "    print(\"samples: \", len(samples))\n",
    "    print(\"s channels: \", len(samples[0].s_channel_list))\n",
    "    print(\"p channels: \", len(samples[0].p_channel_list))\n",
    "    print(\"d channels: \", len(samples[0].d_channel_list))\n",
    "    return samples\n",
    "    \n",
    "def samples2xy(samples):\n",
    "    s_input = np.array([s.s_channels for s in samples], np.float32)\n",
    "    p_input = np.array([s.p_channels for s in samples], np.float32)\n",
    "    d_input = np.array([s.d_channels for s in samples], np.float32)\n",
    "    x_data = [s_input, p_input, d_input]\n",
    "    y_data = np.array([s.xblock for s in samples], np.float32)\n",
    "    return x_data, y_data\n",
    "\n",
    "H_dataset = build_dataset(\"H\", max_l=1)\n",
    "#O_dataset = build_dataset(\"O\", max_l=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract(args):\n",
    "    assert len(args) == 2\n",
    "    coeffs = args[0]\n",
    "    channels = args[1]\n",
    "    return tf.reduce_sum(coeffs[...,None,:] * channels, axis=-1)\n",
    "\n",
    "#TODO implement custom layer instead of Dense + Lambda:\n",
    "\n",
    "def build_model(first_sample, pao_basis_size, prim_basis_shells, num_hidden_layers):\n",
    "    \n",
    "    prim_basis_size = sum([(2*l+1)*n for l, n in enumerate(prim_basis_shells)])\n",
    "    num_p_channels = len(first_sample.p_channel_list)\n",
    "    num_d_channels = len(first_sample.d_channel_list)\n",
    "    \n",
    "    # define two sets of inputs\n",
    "    s_input = layers.Input(shape=first_sample.s_channels.shape, name=\"s_input\")\n",
    "    p_input = layers.Input(shape=first_sample.p_channels.shape, name=\"p_input\")\n",
    "    d_input = layers.Input(shape=first_sample.d_channels.shape, name=\"d_input\")\n",
    "    \n",
    "    #TODO: add more hidden layers\n",
    "    hidden_out = s_input\n",
    "    for i in range(num_hidden_layers):\n",
    "        hidden_out = layers.Dense(8, activation='softmax', name=f\"hidden_layer_{i}\")(hidden_out)\n",
    "            \n",
    "    pao_output = []\n",
    "    for i in range(pao_basis_size):\n",
    "        for l, n in enumerate(prim_basis_shells):\n",
    "            for j in range(n):\n",
    "                name = f\"{i}{l}{j}\"\n",
    "                if l == 0:\n",
    "                    s_out = layers.Dense(1, name=\"coeffs_\"+name)(hidden_out)\n",
    "                    pao_output.append(s_out)\n",
    "                elif l == 1:\n",
    "                    p_coeffs = layers.Dense(num_p_channels, name=\"coeffs_\"+name)(hidden_out)\n",
    "                    contract_layer = layers.Lambda(contract, name=\"contr_\"+name)\n",
    "                    p_out = contract_layer([p_coeffs, p_input])\n",
    "                    pao_output.append(p_out)\n",
    "                elif l == 2:\n",
    "                    d_coeffs = layers.Dense(num_d_channels, name=\"coeffs_\"+name)(hidden_out)\n",
    "                    contract_layer = layers.Lambda(contract, name=\"contr_\"+name)\n",
    "                    d_out = contract_layer([d_coeffs, d_input])\n",
    "                    pao_output.append(d_out)\n",
    "                else:\n",
    "                    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    xvec = layers.concatenate(pao_output)\n",
    "    xblock = layers.Reshape((pao_basis_size, prim_basis_size))(xvec) #TODO: maybe transpose?\n",
    "\n",
    "    inputs = [s_input, p_input, d_input]\n",
    "    model = keras.Model(inputs=inputs, outputs=xblock)\n",
    "    #model.summary()\n",
    "    return(model)\n",
    "\n",
    "H_model = build_model(H_dataset[0], pao_basis_size, prim_basis_shells['H'], num_hidden_layers=2)\n",
    "#O_model = build_model(O_dataset[0], pao_basis_size, prim_basis_shells['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(xblock_true, xblock_pred):\n",
    "    #TODO: This might not be ideal as it implicitly forces the predicted basis vectors to be orthonormal.\n",
    "    projector = tf.matmul(xblock_pred, xblock_pred, transpose_a=True)\n",
    "    residual = xblock_true - tf.matmul(xblock_true, projector)\n",
    "    return tf.reduce_mean(tf.pow(residual, 2))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "H_model.compile(optimizer, loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 1.2102818747194988e-13\n",
      "Epoch: 20 Loss: 6.909750464807523e-15\n",
      "Epoch: 30 Loss: 3.7952280207420586e-14\n",
      "Epoch: 40 Loss: 5.316649965875393e-14\n",
      "Epoch: 50 Loss: 3.358841050887969e-14\n",
      "Epoch: 60 Loss: 2.2103151963879727e-14\n",
      "Epoch: 70 Loss: 4.371433702759879e-14\n",
      "Epoch: 80 Loss: 6.376704584287393e-14\n",
      "Epoch: 90 Loss: 3.4806186389015406e-14\n",
      "Epoch: 100 Loss: 1.464162179085765e-13\n",
      "Epoch: 110 Loss: 2.588901315547787e-14\n",
      "Epoch: 120 Loss: 2.0942276369008764e-14\n",
      "Epoch: 130 Loss: 1.8161860226460422e-14\n",
      "Epoch: 140 Loss: 5.5202370452533955e-14\n",
      "Epoch: 150 Loss: 2.590011606335048e-14\n",
      "Epoch: 160 Loss: 8.388428840433448e-14\n",
      "Epoch: 170 Loss: 1.5729847708655792e-13\n",
      "Epoch: 180 Loss: 1.7152113605291286e-13\n",
      "Epoch: 190 Loss: 1.3816100769958178e-13\n",
      "Epoch: 200 Loss: 1.967224939804918e-13\n",
      "Epoch: 210 Loss: 2.6918744333191344e-14\n",
      "Epoch: 220 Loss: 6.044470479693587e-14\n",
      "Epoch: 230 Loss: 6.043637676899846e-14\n",
      "Epoch: 240 Loss: 2.6578600973749256e-13\n",
      "Epoch: 250 Loss: 9.673511991437067e-14\n",
      "Epoch: 260 Loss: 2.982267143197652e-14\n",
      "Epoch: 270 Loss: 1.4401674297559452e-14\n",
      "Epoch: 280 Loss: 4.08499629793186e-14\n",
      "Epoch: 290 Loss: 3.452307748485692e-14\n",
      "Epoch: 300 Loss: 3.3970744240611334e-14\n",
      "Test loss: 0.7776252603317055\n"
     ]
    }
   ],
   "source": [
    "class TrainingLogger(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_counter = 0\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_counter += 1\n",
    "        if self.epoch_counter % 10 == 0:\n",
    "            print(\"Epoch: {} Loss: {}\".format(self.epoch_counter, logs['loss']))\n",
    "\n",
    "x_train, y_train = samples2xy(H_dataset[:1])\n",
    "x_test, y_test = samples2xy(H_dataset[1:])\n",
    "H_model.fit(x_train, y_train, epochs=300, verbose=0, callbacks=[TrainingLogger()])\n",
    "loss = H_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
