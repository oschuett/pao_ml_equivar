{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pao_utils import parse_pao_file, append_samples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import se3cnn\n",
    "import livelossplot as llp\n",
    "\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from se3cnn.utils import torch_default_dtype\n",
    "import se3cnn.point_utils as point_utils\n",
    "from se3cnn.non_linearities import NormSoftplus\n",
    "from se3cnn.convolution import SE3PointConvolution\n",
    "from se3cnn.blocks.point_norm_block import PointNormBlock \n",
    "from se3cnn.point_kernel import gaussian_radial_function\n",
    "from se3cnn.SO3 import torch_default_dtype\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and parse all .pao files.\n",
    "# Each file corresponds to a molecular configuration, ie. a frame.\n",
    "# Since the system contains multiple atoms, each .pao file contains multiple samples.\n",
    "class PAODataset(object):\n",
    "    def __init__(self, kind_name):\n",
    "        self.kind_name = kind_name\n",
    "        self.sample_coords = []\n",
    "        self.sample_xblocks = []\n",
    "        self.sample_iatoms = []\n",
    "\n",
    "        #TODO split in training and test set\n",
    "        for fn in glob(\"2H2O_MD/frame_*/2H2O_pao44-1_0.pao\"):\n",
    "            kinds, atom2kind, coords, xblocks = parse_pao_file(fn)\n",
    "            for iatom, kind in enumerate(atom2kind):\n",
    "                if kind != self.kind_name:\n",
    "                    continue\n",
    "                rel_coords = coords - coords[iatom,:] # relative coordinates\n",
    "                self.sample_coords.append(rel_coords)\n",
    "                self.sample_xblocks.append(xblocks[iatom])\n",
    "                self.sample_iatoms.append(iatom)\n",
    "\n",
    "        # assuming kinds and atom2kind are the same across whole training data\n",
    "        kinds_enum = list(kinds.keys())\n",
    "        self.kinds_onehot = np.zeros((len(kinds), len(atom2kind)))\n",
    "        for iatom, kind in enumerate(atom2kind):\n",
    "            idx = kinds_enum.index(kind)\n",
    "            self.kinds_onehot[idx, iatom] = 1.0\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # roll central atom to the front\n",
    "        iatom = self.sample_iatoms[idx]\n",
    "        rolled_kinds = np.roll(self.kinds_onehot, shift=-iatom, axis=1)\n",
    "        rolled_coords =  np.roll(self.sample_coords[idx], shift=-iatom, axis=0)  \n",
    "        return rolled_kinds, rolled_coords, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_xblocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAONet(torch.nn.Module):\n",
    "    def __init__(self, num_kinds, pao_basis_size, prim_basis_shells, num_hidden=1, num_radial=4, max_radius=2.5):\n",
    "        super().__init__()\n",
    "        self.num_kinds = num_kinds\n",
    "        self.prim_basis_shells = prim_basis_shells\n",
    "        self.pao_basis_size = pao_basis_size\n",
    "        \n",
    "        features = []\n",
    "        features.append([num_kinds, 0, 0])  # L=0 for atom type as one-hot encoding\n",
    "        features.append([8, 8, 8]) # hidden layer with filters L=0,1,2\n",
    "        features.append([8, 8, 8]) # hidden layer with filters L=0,1,2\n",
    "        features.append([ i * pao_basis_size for i in prim_basis_shells])\n",
    "\n",
    "        nonlinearity = lambda x: torch.log(0.5 * torch.exp(x) + 0.5)\n",
    "        sigma = max_radius / num_radial\n",
    "        radii = torch.linspace(0, max_radius, steps=num_radial, dtype=torch.float64)\n",
    "        radial_function = partial(gaussian_radial_function, sigma=2*sigma)\n",
    "        radii_args = {'radii': radii, 'radial_function': radial_function}\n",
    "        \n",
    "        # Convolutions with Norm nonlinearity layers\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        # features\n",
    "        input_features = [num_kinds, 0, 0]  # L=0 for atom type as one-hot encoding\n",
    "        hidden_features = [8, 8, 8] # hidden layer with filters L=0,1,2\n",
    "        output_features = [i * pao_basis_size for i in prim_basis_shells]\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(PointNormBlock(input_features, hidden_features, activation=nonlinearity, **radii_args))\n",
    "       \n",
    "        # hidden layer\n",
    "        for _ in range(num_hidden):\n",
    "            self.layers.append(PointNormBlock(hidden_features, hidden_features, activation=nonlinearity, **radii_args))\n",
    "       \n",
    "        # output layer\n",
    "        Rs_repr = lambda features: [(m, l) for l, m in enumerate(features)]\n",
    "        self.layers.append(SE3PointConvolution(Rs_repr(hidden_features), Rs_repr(output_features), **radii_args))\n",
    "                        \n",
    "        \n",
    "    def forward(self, input, difference_mat, relative_mask=None):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, difference_mat, relative_mask)\n",
    "        #TODO: things could be much simpler if the network directly returned decoded 2-D xblocks\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror(xblock):\n",
    "    \"\"\" duplicate pao vectors with flipped sign \"\"\"\n",
    "    m, n = xblock.shape # size of pao and prim basis\n",
    "    result = np.zeros((2*m, n))\n",
    "    result[:m,:] = xblock\n",
    "    result[m:,:] = -xblock\n",
    "    return result\n",
    "\n",
    "def align(xblock, ref_xblock):\n",
    "    \"\"\" align xblock onto ref_xblock in-place \"\"\"\n",
    "    m, n = xblock.shape # size of pao and prim basis\n",
    "    \n",
    "    # We can treat sign-flips as permutations by including each basis vector with both signs.\n",
    "    a = mirror(xblock)\n",
    "    b = mirror(ref_xblock)\n",
    "    \n",
    "    # build distance matrix\n",
    "    dist = np.zeros((2*m,2*m))\n",
    "    for i in range(2*m):\n",
    "        for j in range(2*m):\n",
    "            dist[i,j] = norm(a[i,:] - b[j,:])\n",
    "            \n",
    "    # run Hungarian algorithm\n",
    "    row_ind, col_ind = linear_sum_assignment(dist)\n",
    "\n",
    "    # permute pao basis vectors in-place\n",
    "    permutations = 0\n",
    "    for i, j in enumerate(col_ind[:m]):\n",
    "        permutations += int(i != j)\n",
    "        xblock[i,:] = a[j,:]\n",
    "\n",
    "    return permutations # number of permutations, should approach zero as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_xblock(xblock, prim_basis_shells):\n",
    "    \"\"\"Encodes a [num_pao, num_prim] 2D-block into a 1-D array\"\"\"\n",
    "    xvec = []\n",
    "    i = 0\n",
    "    for l, m in enumerate(prim_basis_shells):\n",
    "        n = m * (2 * l + 1)\n",
    "        xvec.append(xblock[:, i:i+n].flatten())\n",
    "        i += n\n",
    "    return np.concatenate(xvec)\n",
    "\n",
    "def decode_xblock(xvec, num_pao, prim_basis_shells):\n",
    "    \"\"\"Decodes a 1-D array into a [num_pao, num_prim] 2-D block.\"\"\"\n",
    "    xblock = []\n",
    "    i = 0\n",
    "    for l, m in enumerate(prim_basis_shells):\n",
    "        n = m * num_pao * (2 * l + 1)\n",
    "        xblock.append(xvec[i:i+n].reshape(num_pao, m * (2 * l + 1)))\n",
    "        i += n\n",
    "    return np.concatenate(xblock, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim = (\"s1\", \"s2\", \"p1x\", \"p1y\", \"p1z\", \"p2x\", \"p2y\", \"p2z\", \"d1xy\", \"d1yz\", \"d1zx\", \"d1xx\", \"d1zz\")\n",
    "xblock = np.array([[\"%s,%i\"%(x,p) for x in prim ] for p in range(4)])\n",
    "#print(xblock)\n",
    "#print(decode_xblock(encode_xblock(xblock, [2, 2, 1]), 4, [2, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.524096 Permutations: 386\n",
      "Epoch: 1 Loss: 0.907484 Permutations: 98\n",
      "Epoch: 2 Loss: 0.534751 Permutations: 62\n",
      "Epoch: 3 Loss: 0.445002 Permutations: 30\n",
      "Epoch: 4 Loss: 0.372032 Permutations: 31\n",
      "Epoch: 5 Loss: 0.329396 Permutations: 30\n",
      "Epoch: 6 Loss: 0.353996 Permutations: 30\n",
      "Epoch: 7 Loss: 0.323675 Permutations: 31\n",
      "Epoch: 8 Loss: 0.303007 Permutations: 31\n",
      "Epoch: 9 Loss: 0.284878 Permutations: 30\n",
      "Epoch: 10 Loss: 0.281584 Permutations: 30\n",
      "Epoch: 11 Loss: 0.294615 Permutations: 32\n",
      "Epoch: 12 Loss: 0.289696 Permutations: 30\n",
      "Epoch: 13 Loss: 0.262677 Permutations: 30\n",
      "Epoch: 14 Loss: 0.264417 Permutations: 30\n",
      "Epoch: 15 Loss: 0.253363 Permutations: 30\n",
      "Epoch: 16 Loss: 0.258627 Permutations: 30\n",
      "Epoch: 17 Loss: 0.249686 Permutations: 30\n",
      "Epoch: 18 Loss: 0.247709 Permutations: 30\n",
      "Epoch: 19 Loss: 0.244559 Permutations: 30\n",
      "Epoch: 20 Loss: 0.241668 Permutations: 30\n",
      "Epoch: 21 Loss: 0.242636 Permutations: 30\n",
      "Epoch: 22 Loss: 0.246483 Permutations: 30\n",
      "Epoch: 23 Loss: 0.242718 Permutations: 30\n",
      "Epoch: 24 Loss: 0.239542 Permutations: 30\n",
      "Epoch: 25 Loss: 0.239813 Permutations: 30\n",
      "Epoch: 26 Loss: 0.232468 Permutations: 30\n",
      "Epoch: 27 Loss: 0.238281 Permutations: 30\n",
      "Epoch: 28 Loss: 0.236970 Permutations: 30\n",
      "Epoch: 29 Loss: 0.230594 Permutations: 30\n",
      "Epoch: 30 Loss: 0.236790 Permutations: 30\n",
      "Epoch: 31 Loss: 0.227077 Permutations: 30\n",
      "Epoch: 32 Loss: 0.228446 Permutations: 30\n",
      "Epoch: 33 Loss: 0.231526 Permutations: 30\n",
      "Epoch: 34 Loss: 0.229537 Permutations: 30\n",
      "Epoch: 35 Loss: 0.228566 Permutations: 30\n",
      "Epoch: 36 Loss: 0.251683 Permutations: 30\n",
      "Epoch: 37 Loss: 0.229307 Permutations: 30\n",
      "Epoch: 38 Loss: 0.228409 Permutations: 30\n",
      "Epoch: 39 Loss: 0.226124 Permutations: 30\n",
      "Epoch: 40 Loss: 0.243193 Permutations: 30\n",
      "Epoch: 41 Loss: 0.260486 Permutations: 30\n",
      "Epoch: 42 Loss: 0.230296 Permutations: 30\n",
      "Epoch: 43 Loss: 0.232277 Permutations: 30\n",
      "Epoch: 44 Loss: 0.257274 Permutations: 30\n",
      "Epoch: 45 Loss: 0.224333 Permutations: 30\n",
      "Epoch: 46 Loss: 0.218585 Permutations: 30\n",
      "Epoch: 47 Loss: 0.220075 Permutations: 30\n",
      "Epoch: 48 Loss: 0.222216 Permutations: 30\n",
      "Epoch: 49 Loss: 0.238486 Permutations: 30\n",
      "Epoch: 50 Loss: 0.230212 Permutations: 30\n",
      "Epoch: 51 Loss: 0.212200 Permutations: 30\n",
      "Epoch: 52 Loss: 0.214659 Permutations: 30\n",
      "Epoch: 53 Loss: 0.223842 Permutations: 30\n",
      "Epoch: 54 Loss: 0.219224 Permutations: 30\n",
      "Epoch: 55 Loss: 0.208085 Permutations: 30\n",
      "Epoch: 56 Loss: 0.242759 Permutations: 30\n",
      "Epoch: 57 Loss: 0.249110 Permutations: 30\n",
      "Epoch: 58 Loss: 0.218360 Permutations: 30\n",
      "Epoch: 59 Loss: 0.213597 Permutations: 30\n",
      "Epoch: 60 Loss: 0.210144 Permutations: 30\n",
      "Epoch: 61 Loss: 0.208603 Permutations: 30\n",
      "Epoch: 62 Loss: 0.212627 Permutations: 30\n",
      "Epoch: 63 Loss: 0.205320 Permutations: 30\n",
      "Epoch: 64 Loss: 0.244556 Permutations: 30\n",
      "Epoch: 65 Loss: 0.215993 Permutations: 30\n",
      "Epoch: 66 Loss: 0.213566 Permutations: 30\n",
      "Epoch: 67 Loss: 0.207835 Permutations: 30\n",
      "Epoch: 68 Loss: 0.205668 Permutations: 30\n",
      "Epoch: 69 Loss: 0.209297 Permutations: 30\n",
      "Epoch: 70 Loss: 0.211170 Permutations: 30\n",
      "Epoch: 71 Loss: 0.213890 Permutations: 30\n",
      "Epoch: 72 Loss: 0.210088 Permutations: 30\n",
      "Epoch: 73 Loss: 0.203125 Permutations: 30\n",
      "Epoch: 74 Loss: 0.215470 Permutations: 30\n",
      "Epoch: 75 Loss: 0.205452 Permutations: 30\n",
      "Epoch: 76 Loss: 0.211006 Permutations: 30\n",
      "Epoch: 77 Loss: 0.203656 Permutations: 30\n",
      "Epoch: 78 Loss: 0.210570 Permutations: 30\n",
      "Epoch: 79 Loss: 0.202525 Permutations: 30\n",
      "Epoch: 80 Loss: 0.211200 Permutations: 30\n",
      "Epoch: 81 Loss: 0.209433 Permutations: 30\n",
      "Epoch: 82 Loss: 0.232545 Permutations: 30\n",
      "Epoch: 83 Loss: 0.206271 Permutations: 30\n",
      "Epoch: 84 Loss: 0.205339 Permutations: 30\n",
      "Epoch: 85 Loss: 0.199878 Permutations: 30\n",
      "Epoch: 86 Loss: 0.230018 Permutations: 30\n",
      "Epoch: 87 Loss: 0.211085 Permutations: 30\n",
      "Epoch: 88 Loss: 0.205403 Permutations: 30\n",
      "Epoch: 89 Loss: 0.195062 Permutations: 30\n",
      "Epoch: 90 Loss: 0.207269 Permutations: 30\n",
      "Epoch: 91 Loss: 0.232563 Permutations: 30\n",
      "Epoch: 92 Loss: 0.210036 Permutations: 30\n",
      "Epoch: 93 Loss: 0.196674 Permutations: 30\n",
      "Epoch: 94 Loss: 0.208174 Permutations: 30\n",
      "Epoch: 95 Loss: 0.197980 Permutations: 30\n",
      "Epoch: 96 Loss: 0.204464 Permutations: 30\n",
      "Epoch: 97 Loss: 0.223738 Permutations: 30\n",
      "Epoch: 98 Loss: 0.200625 Permutations: 30\n",
      "Epoch: 99 Loss: 0.194818 Permutations: 30\n"
     ]
    }
   ],
   "source": [
    "# assuming MOLOPT-DZVP as primary basis set\n",
    "prim_basis_shells = {\n",
    "    'H': [2, 1, 0], # two s-shells, one p-shell, no d-shells\n",
    "    'O': [2, 2, 1], # two s-shells, two p-shells, one d-shell\n",
    "}\n",
    "\n",
    "net = PAONet(num_kinds=2, pao_basis_size=4, prim_basis_shells=prim_basis_shells['O'])\n",
    "net.train()\n",
    "dataset = PAODataset(\"O\")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_permutations = 0\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        kind_onehot, coords, sample_indices = batch    \n",
    "        diff_M = se3cnn.point_utils.difference_matrix(coords)\n",
    "\n",
    "        # forward pass\n",
    "        output_net = net(kind_onehot, diff_M)\n",
    "\n",
    "        # Use Hungarian algorithm to align training data sample to network's output.\n",
    "\n",
    "        output_sample = []\n",
    "        for i, idx in enumerate(sample_indices):  # loop over batch\n",
    "            # We only care about the xblock of the central atom, which we rolled to the front.\n",
    "            xblock_enc_net = output_net[i,:,0]\n",
    "            \n",
    "            # decode xblock returned by network\n",
    "            xblock_net = decode_xblock(xblock_enc_net.detach().numpy(), 4, prim_basis_shells['O'])\n",
    "            \n",
    "            # get xblock from training data\n",
    "            xblock_sample = dataset.sample_xblocks[idx]\n",
    "            \n",
    "            # aligh sample xblock onto xblock outputed by the network\n",
    "            epoch_permutations += align(xblock_sample, xblock_net)\n",
    "            \n",
    "            # encode aligned sample xblock\n",
    "            output_sample.append(encode_xblock(xblock_sample, prim_basis_shells['O']))\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        output_sample = torch.tensor(output_sample)\n",
    "        loss = loss_fn(output_net[:,:,0], output_sample)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %i Loss: %f Permutations: %i\"%(epoch, epoch_loss, epoch_permutations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
