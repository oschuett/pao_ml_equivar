{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pao_tfn_dataset import PAODataset\n",
    "from pao_tfn_net import PAONet\n",
    "import torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming MOLOPT-DZVP as primary basis set\n",
    "prim_basis_shells = {\n",
    "    'H': [2, 1, 0], # two s-shells, one p-shell, no d-shells\n",
    "    'O': [2, 2, 1], # two s-shells, two p-shells, one d-shell\n",
    "}\n",
    "pao_basis_size = 4\n",
    "kind_name = \"O\"\n",
    "\n",
    "dataset = PAODataset(kind_name)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "net = PAONet(num_kinds=len(prim_basis_shells),\n",
    "             pao_basis_size=pao_basis_size,\n",
    "             prim_basis_shells=prim_basis_shells[kind_name],\n",
    "             num_hidden=1)\n",
    "net.train()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_missmatch = 0\n",
    "    epoch_penalty = 0\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        kind_onehot, coords, sample_indices = batch    \n",
    "        diff_M = se3cnn.point_utils.difference_matrix(coords)\n",
    "\n",
    "        # forward pass\n",
    "        output_net = net(kind_onehot, diff_M)\n",
    "\n",
    "        missmatch = torch.tensor(0.0)\n",
    "        penalty = torch.tensor(0.0)\n",
    "        \n",
    "        #TODO: batchify this to speed things up\n",
    "        for i, idx in enumerate(sample_indices):  # loop over batch\n",
    "            # We only care about the xblock of the central atom, which we rolled to the front.\n",
    "            xblock_net = net.decode_xblock(output_net[i,:,0])\n",
    "            \n",
    "            # We penalize non-unit vectors later, but we are not going to rely on it here.\n",
    "            xblock_net_unit = torch.nn.functional.normalize(xblock_net)\n",
    "            #TODO: This might not be ideal as it implicitly foces the pao basis vectors to be orthogonal.\n",
    "            projector = torch.matmul(torch.t(xblock_net_unit), xblock_net_unit)\n",
    "            \n",
    "            xblock_sample = dataset.sample_xblocks[idx]\n",
    "            residual = torch.t(xblock_sample) - torch.matmul(projector, torch.t(xblock_sample))\n",
    "            missmatch += torch.norm(residual)\n",
    "\n",
    "            # penalize non-unit basis vector\n",
    "            penalty = torch.norm(1 - torch.norm(xblock_net, dim=1))\n",
    "               \n",
    "        loss = missmatch + penalty\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_missmatch += missmatch.item()\n",
    "        epoch_penalty += penalty.item()\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %i  Missmatch: %f  Penalty: %f Loss: %f\"%(epoch, epoch_missmatch, epoch_penalty, epoch_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
