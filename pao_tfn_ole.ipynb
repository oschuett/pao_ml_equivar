{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pao_utils import parse_pao_file, append_samples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import se3cnn\n",
    "import livelossplot as llp\n",
    "\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from se3cnn.utils import torch_default_dtype\n",
    "import se3cnn.point_utils as point_utils\n",
    "from se3cnn.non_linearities import NormSoftplus\n",
    "from se3cnn.convolution import SE3PointConvolution\n",
    "from se3cnn.blocks.point_norm_block import PointNormBlock \n",
    "from se3cnn.point_kernel import gaussian_radial_function\n",
    "from se3cnn.SO3 import torch_default_dtype\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and parse all .pao files.\n",
    "# Each file corresponds to a molecular configuration, ie. a frame.\n",
    "# Since the system contains multiple atoms, each .pao file contains multiple samples.\n",
    "class PAODataset(object):\n",
    "    def __init__(self, kind_name):\n",
    "        self.kind_name = kind_name\n",
    "        self.sample_coords = []\n",
    "        self.sample_xblocks = []\n",
    "        self.sample_iatoms = []\n",
    "\n",
    "        #TODO split in training and test set\n",
    "        for fn in glob(\"2H2O_MD/frame_*/2H2O_pao44-1_0.pao\"):\n",
    "            kinds, atom2kind, coords, xblocks = parse_pao_file(fn)\n",
    "            for iatom, kind in enumerate(atom2kind):\n",
    "                if kind != self.kind_name:\n",
    "                    continue\n",
    "                rel_coords = coords - coords[iatom,:] # relative coordinates\n",
    "                self.sample_coords.append(rel_coords)\n",
    "                self.sample_xblocks.append(xblocks[iatom])\n",
    "                self.sample_iatoms.append(iatom)\n",
    "\n",
    "        # assuming kinds and atom2kind are the same across whole training data\n",
    "        kinds_enum = list(kinds.keys())\n",
    "        self.kinds_onehot = np.zeros((len(kinds), len(atom2kind)))\n",
    "        for iatom, kind in enumerate(atom2kind):\n",
    "            idx = kinds_enum.index(kind)\n",
    "            self.kinds_onehot[idx, iatom] = 1.0\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # roll central atom to the front\n",
    "        iatom = self.sample_iatoms[idx]\n",
    "        rolled_kinds = np.roll(self.kinds_onehot, shift=-iatom, axis=1)\n",
    "        rolled_coords =  np.roll(self.sample_coords[idx], shift=-iatom, axis=0)  \n",
    "        return rolled_kinds, rolled_coords, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_xblocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAONet(torch.nn.Module):\n",
    "    def __init__(self, num_kinds, pao_basis_size, prim_basis_shells, num_radial=4, max_radius=2.5):\n",
    "        super().__init__()\n",
    "        self.num_kinds = num_kinds\n",
    "        self.prim_basis_shells = prim_basis_shells\n",
    "        self.pao_basis_size = pao_basis_size\n",
    "        \n",
    "        features = []\n",
    "        features.append([num_kinds, 0, 0])  # L=0 for atom type as one-hot encoding\n",
    "        features.append([8, 8, 8]) # hidden layer with filters L=0,1,2\n",
    "        features.append([8, 8, 8]) # hidden layer with filters L=0,1,2\n",
    "        features.append([ i * pao_basis_size for i in prim_basis_shells])\n",
    "\n",
    "        nonlinearity = lambda x: torch.log(0.5 * torch.exp(x) + 0.5)\n",
    "        sigma = max_radius / num_radial\n",
    "        radii = torch.linspace(0, max_radius, steps=num_radial, dtype=torch.float64)\n",
    "        radial_function = partial(gaussian_radial_function, sigma=2*sigma)\n",
    "        radii_args = {'radii': radii, 'radial_function': radial_function}\n",
    "\n",
    "        # Convolutions with Norm nonlinearity layers\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        # input layer\n",
    "        self.layers.append(PointNormBlock(features[0], features[1], activation=nonlinearity, **radii_args))\n",
    "        \n",
    "        # hidden layer\n",
    "        self.layers.append(PointNormBlock(features[1], features[2], activation=nonlinearity, **radii_args))\n",
    "        \n",
    "        # output layer\n",
    "        Rs_repr = lambda features: [(m, l) for l, m in enumerate(features)]\n",
    "        self.layers.append(SE3PointConvolution(Rs_repr(features[2]), Rs_repr(features[3]), **radii_args))\n",
    "                        \n",
    "        \n",
    "    def forward(self, input, difference_mat, relative_mask=None):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, difference_mat, relative_mask)\n",
    "        return output\n",
    "\n",
    "        #TODO: this could make things a lot simpler:\n",
    "        ## decode network's 1-D output into 2-D xblock with shape [num_pao, num_prim].\n",
    "        #xblock = output.reshape(-1, self.pao_basis_size).transpose()\n",
    "        #return xblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror(xblock):\n",
    "    \"\"\" duplicate pao vectors with flipped sign \"\"\"\n",
    "    m, n = xblock.shape # size of pao and prim basis\n",
    "    result = np.zeros((2*m, n))\n",
    "    result[:m,:] = xblock\n",
    "    result[m:,:] = -xblock\n",
    "    return result\n",
    "\n",
    "def align(xblock, ref_xblock):\n",
    "    \"\"\" align xblock onto ref_xblock in-place \"\"\"\n",
    "    m, n = xblock.shape # size of pao and prim basis\n",
    "    \n",
    "    # We can treat sign-flips as permutations by including each basis vector with both signs.\n",
    "    a = mirror(xblock)\n",
    "    b = mirror(ref_xblock)\n",
    "    \n",
    "    # build distance matrix\n",
    "    dist = np.zeros((2*m,2*m))\n",
    "    for i in range(2*m):\n",
    "        for j in range(2*m):\n",
    "            dist[i,j] = norm(a[i,:] - b[j,:])\n",
    "            \n",
    "    # run Hungarian algorithm\n",
    "    row_ind, col_ind = linear_sum_assignment(dist)\n",
    "\n",
    "    # permute pao basis vectors in-place\n",
    "    permutations = 0\n",
    "    for i, j in enumerate(col_ind[:m]):\n",
    "        permutations += int(i != j)\n",
    "        xblock[i,:] = a[j,:]\n",
    "\n",
    "    return permutations # number of permutations, should approach zero as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_xblock(xblock):\n",
    "    \"\"\"Encodes a [num_pao, num_prim] 2D-block into a 1-D array\"\"\"\n",
    "    return xblock.transpose().flatten()\n",
    "\n",
    "def decode_xblock(xblock, num_pao):\n",
    "    \"\"\"Decodes a 1-D array into a [num_pao, num_prim] 2-D block.\"\"\"\n",
    "    return xblock.reshape(-1, num_pao).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.895328 Permutations: 448\n",
      "Epoch: 1 Loss: 1.952842 Permutations: 308\n",
      "Epoch: 2 Loss: 1.489995 Permutations: 228\n",
      "Epoch: 3 Loss: 1.317527 Permutations: 155\n",
      "Epoch: 4 Loss: 1.129785 Permutations: 119\n",
      "Epoch: 5 Loss: 1.058874 Permutations: 97\n",
      "Epoch: 6 Loss: 0.982946 Permutations: 92\n",
      "Epoch: 7 Loss: 0.950165 Permutations: 89\n",
      "Epoch: 8 Loss: 0.887799 Permutations: 84\n",
      "Epoch: 9 Loss: 0.848267 Permutations: 74\n",
      "Epoch: 10 Loss: 0.799088 Permutations: 73\n",
      "Epoch: 11 Loss: 0.732214 Permutations: 65\n",
      "Epoch: 12 Loss: 0.732075 Permutations: 63\n",
      "Epoch: 13 Loss: 0.779252 Permutations: 65\n",
      "Epoch: 14 Loss: 0.722521 Permutations: 65\n",
      "Epoch: 15 Loss: 0.665482 Permutations: 63\n",
      "Epoch: 16 Loss: 0.646191 Permutations: 61\n",
      "Epoch: 17 Loss: 0.626444 Permutations: 59\n",
      "Epoch: 18 Loss: 0.635396 Permutations: 58\n",
      "Epoch: 19 Loss: 0.624750 Permutations: 57\n",
      "Epoch: 20 Loss: 0.594042 Permutations: 58\n",
      "Epoch: 21 Loss: 0.614875 Permutations: 58\n",
      "Epoch: 22 Loss: 0.612358 Permutations: 60\n",
      "Epoch: 23 Loss: 0.578107 Permutations: 57\n",
      "Epoch: 24 Loss: 0.601254 Permutations: 55\n",
      "Epoch: 25 Loss: 0.571768 Permutations: 55\n",
      "Epoch: 26 Loss: 0.580308 Permutations: 58\n",
      "Epoch: 27 Loss: 0.530310 Permutations: 56\n",
      "Epoch: 28 Loss: 0.525866 Permutations: 51\n",
      "Epoch: 29 Loss: 0.526976 Permutations: 53\n",
      "Epoch: 30 Loss: 0.561442 Permutations: 58\n",
      "Epoch: 31 Loss: 0.539565 Permutations: 58\n",
      "Epoch: 32 Loss: 0.531205 Permutations: 55\n",
      "Epoch: 33 Loss: 0.499766 Permutations: 53\n",
      "Epoch: 34 Loss: 0.503757 Permutations: 51\n",
      "Epoch: 35 Loss: 0.513376 Permutations: 56\n",
      "Epoch: 36 Loss: 0.559043 Permutations: 55\n",
      "Epoch: 37 Loss: 0.545693 Permutations: 58\n",
      "Epoch: 38 Loss: 0.549216 Permutations: 57\n",
      "Epoch: 39 Loss: 0.499748 Permutations: 57\n",
      "Epoch: 40 Loss: 0.489503 Permutations: 53\n",
      "Epoch: 41 Loss: 0.494637 Permutations: 56\n",
      "Epoch: 42 Loss: 0.492041 Permutations: 56\n",
      "Epoch: 43 Loss: 0.485151 Permutations: 54\n",
      "Epoch: 44 Loss: 0.470715 Permutations: 54\n",
      "Epoch: 45 Loss: 0.462495 Permutations: 51\n",
      "Epoch: 46 Loss: 0.454249 Permutations: 51\n",
      "Epoch: 47 Loss: 0.480774 Permutations: 54\n",
      "Epoch: 48 Loss: 0.511470 Permutations: 55\n",
      "Epoch: 49 Loss: 0.471938 Permutations: 55\n",
      "Epoch: 50 Loss: 0.460529 Permutations: 53\n",
      "Epoch: 51 Loss: 0.461760 Permutations: 50\n",
      "Epoch: 52 Loss: 0.499634 Permutations: 55\n",
      "Epoch: 53 Loss: 0.465183 Permutations: 57\n",
      "Epoch: 54 Loss: 0.467140 Permutations: 57\n",
      "Epoch: 55 Loss: 0.433318 Permutations: 49\n",
      "Epoch: 56 Loss: 0.442842 Permutations: 49\n",
      "Epoch: 57 Loss: 0.432427 Permutations: 49\n",
      "Epoch: 58 Loss: 0.462231 Permutations: 55\n",
      "Epoch: 59 Loss: 0.441952 Permutations: 52\n",
      "Epoch: 60 Loss: 0.433174 Permutations: 44\n",
      "Epoch: 61 Loss: 0.429742 Permutations: 44\n",
      "Epoch: 62 Loss: 0.449091 Permutations: 40\n",
      "Epoch: 63 Loss: 0.403666 Permutations: 37\n",
      "Epoch: 64 Loss: 0.400165 Permutations: 34\n",
      "Epoch: 65 Loss: 0.399925 Permutations: 32\n",
      "Epoch: 66 Loss: 0.371997 Permutations: 30\n",
      "Epoch: 67 Loss: 0.415491 Permutations: 35\n",
      "Epoch: 68 Loss: 0.401932 Permutations: 35\n",
      "Epoch: 69 Loss: 0.392237 Permutations: 35\n",
      "Epoch: 70 Loss: 0.374568 Permutations: 35\n",
      "Epoch: 71 Loss: 0.376408 Permutations: 35\n",
      "Epoch: 72 Loss: 0.379029 Permutations: 35\n",
      "Epoch: 73 Loss: 0.362683 Permutations: 35\n",
      "Epoch: 74 Loss: 0.370232 Permutations: 37\n",
      "Epoch: 75 Loss: 0.363366 Permutations: 32\n",
      "Epoch: 76 Loss: 0.359600 Permutations: 32\n",
      "Epoch: 77 Loss: 0.348924 Permutations: 34\n",
      "Epoch: 78 Loss: 0.398308 Permutations: 34\n",
      "Epoch: 79 Loss: 0.357368 Permutations: 35\n",
      "Epoch: 80 Loss: 0.358622 Permutations: 35\n",
      "Epoch: 81 Loss: 0.351368 Permutations: 30\n",
      "Epoch: 82 Loss: 0.349841 Permutations: 33\n",
      "Epoch: 83 Loss: 0.332341 Permutations: 33\n",
      "Epoch: 84 Loss: 0.336411 Permutations: 33\n",
      "Epoch: 85 Loss: 0.335568 Permutations: 35\n",
      "Epoch: 86 Loss: 0.356469 Permutations: 35\n",
      "Epoch: 87 Loss: 0.331435 Permutations: 35\n",
      "Epoch: 88 Loss: 0.340889 Permutations: 33\n",
      "Epoch: 89 Loss: 0.361856 Permutations: 37\n",
      "Epoch: 90 Loss: 0.361953 Permutations: 32\n",
      "Epoch: 91 Loss: 0.324876 Permutations: 34\n",
      "Epoch: 92 Loss: 0.330910 Permutations: 30\n",
      "Epoch: 93 Loss: 0.372361 Permutations: 35\n",
      "Epoch: 94 Loss: 0.349801 Permutations: 37\n",
      "Epoch: 95 Loss: 0.312886 Permutations: 34\n",
      "Epoch: 96 Loss: 0.323124 Permutations: 30\n",
      "Epoch: 97 Loss: 0.317244 Permutations: 30\n",
      "Epoch: 98 Loss: 0.321855 Permutations: 30\n",
      "Epoch: 99 Loss: 0.332627 Permutations: 33\n"
     ]
    }
   ],
   "source": [
    "# assuming MOLOPT-DZVP as primary basis set\n",
    "prim_basis_shells = {\n",
    "    'H': [2, 1, 0], # two s-shells, one p-shell, no d-shells\n",
    "    'O': [2, 2, 1], # two s-shells, two p-shells, one d-shell\n",
    "}\n",
    "\n",
    "net = PAONet(num_kinds=2, pao_basis_size=4, prim_basis_shells=prim_basis_shells['O'])\n",
    "net.train()\n",
    "dataset = PAODataset(\"O\")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_permutations = 0\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        kind_onehot, coords, sample_indices = batch    \n",
    "        diff_M = se3cnn.point_utils.difference_matrix(coords)\n",
    "\n",
    "        # forward pass\n",
    "        output_net = net(kind_onehot, diff_M)\n",
    "\n",
    "        # Use Hungarian algorithm to align training data sample to network's output.\n",
    "\n",
    "        output_sample = []\n",
    "        for i, idx in enumerate(sample_indices):  # loop over batch\n",
    "            # We only care about the xblock of the central atom, which we rolled to the front.\n",
    "            xblock_enc_net = output_net[i,:,0]\n",
    "            \n",
    "            # decode xblock returned by network\n",
    "            xblock_net = decode_xblock(xblock_enc_net.detach().numpy(), num_pao=4)\n",
    "            \n",
    "            # get xblock from training data\n",
    "            xblock_sample = dataset.sample_xblocks[idx]\n",
    "            \n",
    "            # aligh sample xblock onto xblock outputed by the network\n",
    "            epoch_permutations += align(xblock_sample, xblock_net)\n",
    "            \n",
    "            # encode aligned sample xblock\n",
    "            output_sample.append(encode_xblock(xblock_sample))\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        output_sample = torch.tensor(output_sample)\n",
    "        loss = loss_fn(output_net[:,:,0], output_sample)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %i Loss: %f Permutations: %i\"%(epoch, epoch_loss, epoch_permutations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
