{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Field Networks\n",
    "\n",
    "Implementation of shape classification demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from math import pi, sqrt\n",
    "import tensorfieldnetworks.layers as layers\n",
    "import tensorfieldnetworks.utils as utils\n",
    "from tensorfieldnetworks.utils import FLOAT_TYPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]], shape=(2, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=254, shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = tf.Variable(tf.ones(shape=(2,2)), name=\"W\")\n",
    "b = tf.Variable(tf.zeros(shape=(2)), name=\"b\")\n",
    "\n",
    "@tf.function\n",
    "def forward(x):\n",
    "  return W * x + b\n",
    "\n",
    "out_a = forward([1,0])\n",
    "print(out_a)\n",
    "tf.zeros(shape=(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def R(inputs, nonlin=tf.nn.relu, hidden_dim=None, output_dim=1, weights_initializer=None, biases_initializer=None):\n",
    "    \"\"\" Computes: radial = b2 + w2 * nonlin(b1 + w1 * input) \"\"\"\n",
    "\n",
    "        if weights_initializer is None:\n",
    "            weights_initializer = tf.initializers.GlorotNormal()\n",
    "        if biases_initializer is None:\n",
    "            biases_initializer = tf.initializers.Constant(0.)\n",
    "\n",
    "        input_dim = inputs.get_shape()[-1]\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = input_dim\n",
    "\n",
    "        w1 = tf.Variable(name='weights1', [hidden_dim, input_dim], dtype=FLOAT_TYPE,\n",
    "                             initializer=weights_initializer)\n",
    "        b1 = tf.Variable(name='biases1', [hidden_dim], dtype=FLOAT_TYPE, initializer=biases_initializer)\n",
    "\n",
    "        w2 = tf.Variable(name='weights2', [output_dim, hidden_dim], dtype=FLOAT_TYPE,\n",
    "                             initializer=weights_initializer)\n",
    "        b2 = tf.Variable(name='biases2', [output_dim], dtype=FLOAT_TYPE, initializer=biases_initializer)\n",
    "\n",
    "        hidden_layer = nonlin(b1 + tf.tensordot(inputs, w1, [[2], [1]]))\n",
    "        radial = b2 + tf.tensordot(hidden_layer, w2, [[2], [1]])\n",
    "\n",
    "        # [N, N, output_dim]\n",
    "        return radial\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def F_0(inputs, nonlin=tf.nn.relu, hidden_dim=None, output_dim=1):\n",
    "    # [N, N, output_dim, 1]\n",
    "\n",
    "    return tf.expand_dims(\n",
    "        R(inputs, nonlin=nonlin, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "          weights_initializer=weights_initializer, biases_initializer=biases_initializer),\n",
    "        axis=-1)\n",
    "\n",
    "    \n",
    "@tf.function\n",
    "def filter_0(layer_input, rbf_inputs, nonlin=tf.nn.relu, hidden_dim=None, output_dim=1):\n",
    "    # [N, N, output_dim, 1]\n",
    "    F_0_out = F_0(rbf_inputs, nonlin=nonlin, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    # [N, output_dim]\n",
    "    input_dim = layer_input.get_shape().as_list()[-1]\n",
    "    # Expand filter axis \"j\"\n",
    "    cg = tf.expand_dims(tf.eye(input_dim), axis=-2)\n",
    "    # L x 0 -> L\n",
    "    return tf.einsum('ijk,abfj,bfk->afi', cg, F_0_out, layer_input)\n",
    "\n",
    "@tf.function\n",
    "def convolution(inputs, rbf, unit_vectors):\n",
    "    for l, input_l in enu\n",
    "    output_tensor_list = {0: [], 1: []}\n",
    "\n",
    "    # loop over l\n",
    "    for key in input_tensor_list:\n",
    "        with tf.variable_scope(f\"L{key}\"):\n",
    "\n",
    "            # loop over atom\n",
    "            for i, tensor in enumerate(input_tensor_list[key]):\n",
    "                output_dim = tensor.get_shape().as_list()[-2]  # number of channels\n",
    "\n",
    "                # L x 0 -> L\n",
    "                tensor_out = filter_0(tensor, rbf, output_dim=output_dim)\n",
    "                m = 0 if tensor_out.get_shape().as_list()[-1] == 1 else 1\n",
    "                output_tensor_list[m].append(tensor_out)\n",
    "\n",
    "        return output_tensor_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radial basis functions\n",
    "rbf_low = 0.0\n",
    "rbf_high = 3.5\n",
    "rbf_count = 4\n",
    "rbf_spacing = (rbf_high - rbf_low) / rbf_count\n",
    "centers = tf.cast(tf.linspace(rbf_low, rbf_high, rbf_count), FLOAT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [<tf.Tensor 'Const_7:0' shape=(1, 1) dtype=float32>, <tf.Tensor 'Const_8:0' shape=(1, 1) dtype=float32>, <tf.Tensor 'Const_9:0' shape=(1, 1) dtype=float32>, <tf.Tensor 'Const_10:0' shape=(1, 1) dtype=float32>, <tf.Tensor 'Const_11:0' shape=(1, 1) dtype=float32>, <tf.Tensor 'Const_12:0' shape=(1, 1) dtype=float32>]}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable convolution/L0/tensor_0/F0_to_L/F_0/radial_function/weights1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/workspace/tensorfieldnetworks/layers.py\", line 23, in R\n    initializer=weights_initializer)\n  File \"/workspace/tensorfieldnetworks/layers.py\", line 68, in F_0\n    weights_initializer=weights_initializer, biases_initializer=biases_initializer),\n  File \"/workspace/tensorfieldnetworks/layers.py\", line 113, in filter_0\n    weights_initializer=weights_initializer, biases_initializer=biases_initializer)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-ce30f48eb6ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0moutput_tensor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/tensorfieldnetworks/layers.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input_tensor_list, rbf, unit_vectors, weights_initializer, biases_initializer)\u001b[0m\n\u001b[1;32m    262\u001b[0m                                                   \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                                                   \u001b[0mweights_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                                                   biases_initializer=biases_initializer)\n\u001b[0m\u001b[1;32m    265\u001b[0m                             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensor_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                             \u001b[0mtensor_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F0_to_L_out_tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/tensorfieldnetworks/layers.py\u001b[0m in \u001b[0;36mfilter_0\u001b[0;34m(layer_input, rbf_inputs, nonlin, hidden_dim, output_dim, weights_initializer, biases_initializer)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# [N, N, output_dim, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         F_0_out = F_0(rbf_inputs, nonlin=nonlin, hidden_dim=hidden_dim, output_dim=output_dim,\n\u001b[0;32m--> 113\u001b[0;31m                       weights_initializer=weights_initializer, biases_initializer=biases_initializer)\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;31m# [N, output_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/tensorfieldnetworks/layers.py\u001b[0m in \u001b[0;36mF_0\u001b[0;34m(inputs, nonlin, hidden_dim, output_dim, weights_initializer, biases_initializer)\u001b[0m\n\u001b[1;32m     66\u001b[0m         return tf.expand_dims(\n\u001b[1;32m     67\u001b[0m             R(inputs, nonlin=nonlin, hidden_dim=hidden_dim, output_dim=output_dim,\n\u001b[0;32m---> 68\u001b[0;31m               weights_initializer=weights_initializer, biases_initializer=biases_initializer),\n\u001b[0m\u001b[1;32m     69\u001b[0m             axis=-1)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/tensorfieldnetworks/layers.py\u001b[0m in \u001b[0;36mR\u001b[0;34m(inputs, nonlin, hidden_dim, output_dim, weights_initializer, biases_initializer)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         w1 = tf.get_variable('weights1', [hidden_dim, input_dim], dtype=FLOAT_TYPE,\n\u001b[0;32m---> 23\u001b[0;31m                              initializer=weights_initializer)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'biases1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_TYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbiases_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable convolution/L0/tensor_0/F0_to_L/F_0/radial_function/weights1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/workspace/tensorfieldnetworks/layers.py\", line 23, in R\n    initializer=weights_initializer)\n  File \"/workspace/tensorfieldnetworks/layers.py\", line 68, in F_0\n    weights_initializer=weights_initializer, biases_initializer=biases_initializer),\n  File \"/workspace/tensorfieldnetworks/layers.py\", line 113, in filter_0\n    weights_initializer=weights_initializer, biases_initializer=biases_initializer)\n"
     ]
    }
   ],
   "source": [
    "# r : [N, 3]\n",
    "r = tf.placeholder(FLOAT_TYPE, shape=(4, 3))\n",
    "\n",
    "# rij : [N, N, 3]\n",
    "rij = utils.difference_matrix(r)\n",
    "\n",
    "# dij : [N, N]\n",
    "dij = utils.distance_matrix(r)\n",
    "\n",
    "# rbf : [N, N, rbf_count]\n",
    "gamma = 1. / rbf_spacing\n",
    "rbf = tf.exp(-gamma * tf.square(tf.expand_dims(dij, axis=-1) - centers))\n",
    "\n",
    "layer_dims = [1, 4, 4, 4]\n",
    "num_layers = len(layer_dims) - 1\n",
    "\n",
    "# embed : [N, layer1_dim, 1]\n",
    "with tf.variable_scope(None, \"embed\"):\n",
    "    embed = layers.self_interaction_layer_without_biases(tf.ones(shape=(4, 1, 1)), layer_dims[0])\n",
    "\n",
    "# should use tf.one_hot, but for only two kinds it's not needed\n",
    "#                          O       H       H      O       H       H\n",
    "input_tensor_list = {0: list([ tf.constant([[v]], tf.float32) for v in [1,0,0,1,0,0] ])}\n",
    "                         \n",
    "print(input_tensor_list)\n",
    "output_tensor_list = layers.convolution(input_tensor_list, rbf, rij)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(output_tensor_list)\n",
    "# for layer, layer_dim in enumerate(layer_dims[1:]):\n",
    "#     with tf.variable_scope(None, 'layer' + str(layer), values=[input_tensor_list]):\n",
    "#         input_tensor_list = layers.convolution(input_tensor_list, rbf, rij)\n",
    "#         input_tensor_list = layers.concatenation(input_tensor_list)\n",
    "#         input_tensor_list = layers.self_interaction(input_tensor_list, layer_dim)\n",
    "#         input_tensor_list = layers.nonlinearity(input_tensor_list)\n",
    "\n",
    "# tfn_scalars = input_tensor_list[0][0]\n",
    "# tfn_output_shape = tfn_scalars.get_shape().as_list()\n",
    "# tfn_output = tf.reduce_mean(tf.squeeze(tfn_scalars), axis=0)\n",
    "# fully_connected_layer = tf.get_variable('fully_connected_weights', \n",
    "#                                         [tfn_output_shape[-2], len(dataset)], dtype=FLOAT_TYPE)\n",
    "# output_biases = tf.get_variable('output_biases', [len(dataset)], dtype=FLOAT_TYPE)\n",
    "\n",
    "# # output : [num_classes]\n",
    "# output = tf.einsum('xy,x->y', fully_connected_layer, tfn_output) + output_biases\n",
    "\n",
    "# tf_label = tf.placeholder(tf.int32)\n",
    "\n",
    "# # truth : [num_classes]\n",
    "# truth = tf.one_hot(tf_label, num_classes)\n",
    "\n",
    "# # loss : []\n",
    "# loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=truth, logits=output)\n",
    "\n",
    "# optim = tf.train.AdamOptimizer(learning_rate=1.e-3)\n",
    "\n",
    "# train_op = optim.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0]], [[2.0]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[[1.]], [[2.]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n",
      "['O', 'H', 'H', 'O', 'H', 'H']\n"
     ]
    }
   ],
   "source": [
    "# parse training data:\n",
    "from glob import glob\n",
    "from pao_file_utils import parse_pao_file\n",
    "\n",
    "class Geometry:\n",
    "    def __init__(self, coords):\n",
    "        self.coords = coords\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self, geo, iatom, xblock):\n",
    "        self.geo = geo\n",
    "        self.iatom = iatom\n",
    "        self.xblock = xblock\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, samples, atom2kind, kinds):\n",
    "        self.samples = samples\n",
    "        self.atom2kind = atom2kind\n",
    "        self.kinds = kinds\n",
    "\n",
    "samples = []\n",
    "pao_files = sorted(glob(\"2H2O_MD/frame_*/2H2O_pao44-1_0.pao\"))\n",
    "for fn in pao_files:\n",
    "    kinds, atom2kind, coords, xblocks = parse_pao_file(fn)\n",
    "    geo = Geometry(coords)\n",
    "    for iatom, xblock in enumerate(xblocks):\n",
    "        s = Sample(geo, iatom, xblock)\n",
    "        samples.append(s)\n",
    "\n",
    "dataset = Dataset(samples, atom2kind, kinds)  # Assuming kinds are the consistend across dataset.\n",
    "print(len(dataset.samples))\n",
    "print(dataset.atom2kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: validation loss = 2.115\n",
      "Epoch 100: validation loss = 0.935\n",
      "Epoch 200: validation loss = 0.090\n",
      "Epoch 300: validation loss = 0.022\n",
      "Epoch 400: validation loss = 0.011\n",
      "Epoch 500: validation loss = 0.006\n",
      "Epoch 600: validation loss = 0.004\n",
      "Epoch 700: validation loss = 0.002\n",
      "Epoch 800: validation loss = 0.000\n",
      "Epoch 900: validation loss = 0.000\n",
      "Epoch 1000: validation loss = 0.000\n",
      "Epoch 1100: validation loss = 0.000\n",
      "Epoch 1200: validation loss = 0.000\n",
      "Epoch 1300: validation loss = 0.000\n",
      "Epoch 1400: validation loss = 0.000\n",
      "Epoch 1500: validation loss = 0.000\n",
      "Epoch 1600: validation loss = 0.000\n",
      "Epoch 1700: validation loss = 0.000\n",
      "Epoch 1800: validation loss = 0.000\n",
      "Epoch 1900: validation loss = 0.000\n",
      "Epoch 2000: validation loss = 0.000\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 2001\n",
    "print_freq = 100\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# training\n",
    "for epoch in range(max_epochs):    \n",
    "    loss_sum = 0.\n",
    "    for label, shape in enumerate(dataset):\n",
    "        loss_value, _ = sess.run([loss, train_op], feed_dict={r: shape, tf_label: label})\n",
    "        loss_sum += loss_value\n",
    "        \n",
    "    if epoch % print_freq == 0:\n",
    "        print(\"Epoch %d: validation loss = %.3f\" % (epoch, loss_sum / len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState()\n",
    "test_set_size = 25\n",
    "predictions = [list() for i in range(len(dataset))]\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "for i in range(test_set_size):\n",
    "    for label, shape in enumerate(dataset):\n",
    "        rotation = utils.random_rotation_matrix(rng)\n",
    "        rotated_shape = np.dot(shape, rotation)\n",
    "        translation = np.expand_dims(np.random.uniform(low=-3., high=3., size=(3)), axis=0)\n",
    "        translated_shape = rotated_shape + translation\n",
    "        output_label = sess.run(tf.argmax(output), \n",
    "                                feed_dict={r: rotated_shape, tf_label: label})\n",
    "        total_predictions += 1\n",
    "        if output_label == label:\n",
    "            correct_predictions += 1\n",
    "print('Test accuracy: %f' % (float(correct_predictions) / total_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
